{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip install tensorflow-probability==0.15.0 pillow "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import tensorflow_probability as tfp\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "codings_size = 128\n",
    "\n",
    "generator, discriminator = Generator(), Discriminator()\n",
    "generator_optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.5)\n",
    "discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.5)\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "sample_array = tf.convert_to_tensor(np.array(np.meshgrid(np.arange(28), np.arange(28))).T.reshape(-1, 2))\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './training_checkpoints', max_to_keep=3)\n",
    "\n",
    "# @tf.function\n",
    "def plot(points):\n",
    "    img = Image.new('L', (28, 28))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.line(points.tolist(), fill=255, width=2)\n",
    "\n",
    "    return (np.array(img, dtype=\"float32\").reshape(28, 28, 1) - 127.5) / 127.5\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(None, tf.int64)])\n",
    "def tf_function(input):\n",
    "    return tf.numpy_function(plot, [input], tf.float32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample_trajectories(probability):\n",
    "    probability_trajectories = tf.split(probability, 5)\n",
    "    dist = tfp.distributions.Categorical(probs=probability_trajectories)\n",
    "    trajectory_index = dist.sample()\n",
    "\n",
    "    trajectory_probability = tf.gather(probability_trajectories, trajectory_index, axis=1, batch_dims=1)\n",
    "    trajectory_loss = tf.math.reduce_sum(tf.math.log(trajectory_probability))\n",
    "    trajectory = tf.gather(sample_array, trajectory_index, axis=0, batch_dims=1)\n",
    "\n",
    "    return tf.reshape(trajectory, [-1]), tf.expand_dims(trajectory_loss, 0)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def generator_loss(g_out, d_out):\n",
    "    g_loss = tf.math.multiply(g_out, cross_entropy(tf.ones_like(d_out), d_out))\n",
    "    return tf.math.reduce_mean(g_loss)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        probability_trajectories = generator(noise, training=True)\n",
    "\n",
    "        trajectory_index, loss_list = tf.vectorized_map(sample_trajectories, probability_trajectories, False)\n",
    "        fake_img = tf.vectorized_map(tf_function, trajectory_index)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(tf.reshape(fake_img,(batch_size, 28, 28, 1)), training=True)\n",
    "\n",
    "        g_loss = generator_loss(loss_list, fake_output)\n",
    "        d_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return g_loss, d_loss, real_output, fake_output\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "(X_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "# Scale the pixel intensities down to the [0,1] range by dividing them by 255.0\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "\n",
    "# Creating a Dataset to iterate through the images\n",
    "train_filter = np.where((y_train == 1))\n",
    "X_train, y_train = X_train[train_filter], y_train[train_filter]\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "def train_gan(dataset, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_num, X_batch in dataset.enumerate():\n",
    "            # phase 1 : training the discriminator\n",
    "\n",
    "            # if batch_num % 100 == 0:\n",
    "\n",
    "\n",
    "            g_loss, d_loss, real_output, fake_output = train_step(X_batch)\n",
    "            if batch_num % 1500 == 0:\n",
    "                print(f\"Epoch : {epoch}, g_loss:{g_loss}  d_loss:{d_loss} {real_output} {fake_output}\")\n",
    "                noise = tf.random.normal(shape=[1, codings_size])\n",
    "                probability = generator.predict(noise)\n",
    "                points = sample_trajectories(probability[0])[0].numpy()\n",
    "                img = Image.new('L', (28, 28))\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.line(points.tolist(), fill=255, width=2)\n",
    "                img.save(f\"./output/img-{batch_num}.jpeg\")\n",
    "        if epoch % 15==0:\n",
    "            manager.save()\n",
    "\n",
    "\n",
    "\n",
    "n_epochs = 5000\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "train_gan(dataset, n_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}